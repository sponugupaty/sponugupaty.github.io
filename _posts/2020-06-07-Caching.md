# Technology : Caching

Here's the table of contents:

1. TOC
{:toc}

## Overview

- Caches take advantage of the locality of reference principle: recently requested data is likely to be requested again
- used in almost every layer of computing: hardware, operating systems, web browsers, web applications, and more
- is like short-term memory: it has a limited amount of space, but is typically faster than the original data source 
- And contains the most recently accessed items
- often found at the level nearest to the front end

**Key reasons for using a cache**
- Using commonly used data like user profile. Save network calls 
- To avoid expensive computations . Example - average age of all users
- To avoid load on the database.

- Not every piece of data should be cached since the hardware running the cache is generally expensive ( SSD's)
- Search times within the cache will increase if we store everything
- LRU - most recently at the top 
- If not of enough size - can lead to thrashing ( constant reading and writing from cache without using it )
- Can be placed close to the servers or close to the database 
- Advantage of closer to the server - Faster , Simpler to implement 
- Advantage of a global cache - fine even when server fails , can scale independently ,

- **Suggestions of what to cache:**
	- User sessions
	- Fully rendered web pages
	- Activity streams
	- User graph data


**Application server cache**
- Placing a cache on the request layer node is one option
- It can quickly respond to a query using the local cache 
- if not available in cache , it can go the local disk and respond
- Cache can exist in local memory ( very fast ) or on local drive ( faster than going to network drive )
- Possible to have each node have its own cache 
	- Issue - if load balancer randomly distributes the load , leads to high cache misses 
	- Solution - global caches and distributed caches 
	
**Content Distribution Network (CDN)**
- CDNs are a kind of cache that comes into play for sites serving large amounts of static media
- static media is asked for to the CDN 
	- if available - serve it locally 
	- else request to backend server , cache it and then serve back 
- if not big enough to have CDN , use subdomain for starters and then expand to CDN

**Cache Invalidation**
- While useful , we still need to make sure that the data is coherent with the data in DB( source of truth )
- if data in DB is updated , the data in cache invalidated in cache 
- if not, this causes inconsistent behavior 
- this is called cache invalidation 
- Four schemes primarily used 
   - **Write aside** - Application responsible for reading and writing from cache 
   - The cache doesnt interact directly with the storage
   - Steps are 
     - Application looks for entry in cache resulting in cache miss
     - load entry from DB 
     - add entry to cache 
     - return entry 
   - Memcached is typically used in this manner 
   - Disadvantages 
     - Cache miss results in 3 trips causing noticeable delay
     - Data can become stale if updated in DB . Workaround - TTL
     - If the node crashes , replaced by new empty node 
   - **Write Through** - Application uses cache as main data store 
   - Cache responsible for writing to data store 
   - Slow overall operation due to write 
   - But subsequent read operations are fast 
   - Users more tolerant of latency while updating than reading 
   - Data in cache is never stale 
   - Disadvantages 
	- if node crashes , the new node will not cache till DB is updated 
	- Most data written will never be read - minimised with TTL 
   - **Write-Behind** - Add/Update entry in cache .Asynchroously write data to datastore 
   - disadvantages 
	- data loss if the node goes down prior to data getting updated in datastore 
     	- more complex to implement than the prior 2
   - **Refresh Ahead**
   - Predict which items are needed and refresh from the data store 
   - Correct prediction needed 

**References**
- [DonnerMartin](https://github.com/donnemartin/system-design-primer#cache)
- [Educative.io](https://educative.io/)
	
## Memcache

- **What is Memcache ?**
	- In-memory key value data store
	- Faster than the disk based caches 
	- Can put anything that is serializable as either keys or values
- **What is Memcache useful for ?**
	- Caching
		- Data store query results 
		- User authentication or session data 
		- API calls or other computation results 
	- Sharing data across app instances 
- **Why do we need Memcache ?**
	- Improve Application performance
		- Faster than datastore  
	- Reduce application cost 
- **How do we use Memcache ?**
	- Usage Pattern -
		- Co-ordinate data read with data store -
			- Check if memcache value exists - if yes - show directly 
			- If does not - fetch from data store and write value to memcache 
		- Co-ordinate data write with data store -
			- Invalidate the entire cache or particular entry 
			- Write to data store 
			- Optionally update memcache entry 
- **Caveats of Memcache -**
	- It is volatile -
		- Entry has reached expiration 
		- Memcache is full 
		- Memcache server fails 
		- Solution : Implement write-through logic to data store
	- It is not transactional -
		- Causing inconsistency in data 
		- Solution : Use optimistic locking
	- It is a limited resource 
- **Memcache vs Redis comparison:**
	- Both can perform the same functions 
	- Read/Write speed : Redis faster 
	- Memory Usage : Redis is better 
	- Disk I/O Dumping : Redis
	- Memcache Value to be stored limited to 1MB . Redis 512MB
	- Memcache looses data on restart . Redis can retain data 
	- Redis comes with cluster support 
	- Redis has support for many data types whereas memcache is primarily strings

**References**
- [YouTube Link](https://www.youtube.com/watch?v=TGl81wr8lz8)
- [StackOverflow](https://stackoverflow.com/questions/10558465/memcached-vs-redis)
- [Redis](https://www.credera.com/blog/technology-insights/java/redis-explained-5-minutes-less/)

---

## Cache Eviction Policies
- First In First Out (FIFO): The cache evicts the first block accessed first without any regard to how often or how many times it was accessed before.
- Last In First Out (LIFO): The cache evicts the block accessed most recently first without any regard to how often or how many times it was accessed before.
- Least Recently Used (LRU): Discards the least recently used items first.
- Most Recently Used (MRU): Discards, in contrast to LRU, the most recently used items first.
- Least Frequently Used (LFU): Counts how often an item is needed. Those that are used least often are discarded first.
- Random Replacement (RR): Randomly selects a candidate item and discards it to make space when necessary.

## Disadvantage(s): cache
- Need to maintain consistency between caches and the source of truth such as the database through cache invalidation.
- Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache.
- Need to make application changes such as adding Redis or memcached.

